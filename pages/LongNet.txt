 Yesterday July 5th this paper dropped long net scaling Transformers to 1 billion tokens now to put this in context this is a three gigapixel image which you can make sense of at a glance and I'm not going to dig too deep into the uh the cognitive neuroscience and neurological mechanisms about why you can make sense of so much information so quickly but if you want to learn more about that I recommend the forgetting machine but what I want to point out is that you
 can take a glance at this image and then you can zoom in and you understand the implications of every little bit of this image this is clearly an arid mountain range there's a road going across it there's uh some Haze there's a city in the background you can keep track of all of that information at once just by glancing at this image and then when you zoom in you can say oh look there's a nice house on the hillside and you can keep track of that information in the context of this three gigapixel image
 this is fundamentally what sparse attention is and that is how this paper solves the problem of reading a billion tokens so let's unpack this a little bit first I love this chart this is this is a really hilarious Flex right at the beginning of this paper right under the abstract they're like okay you know 512 uh 12K 64k tokens uh 262 a million tokens and then here's us a billion tokens so good job oh also I want to point out this is from Microsoft this is not just from some Podunk you know Backwater University this is Microsoft
 you know who's in partnership with openai uh and so I saw a post somewhere I think it was a tweet or something someone's like Microsoft seems like they're really just falling down on AI research and I have no idea what rock they're living under but pay attention to Microsoft my money is on Microsoft and Nvidia uh for for the AI race and then of course there's Google um but I don't understand get Google's business model because they invented this stuff and then sat on it for seven
 years so I have no idea what Google's doing anyways sorry I digress okay billion tokens seems kind of out there kind of hyperbolic right the the chief uh Innovation here is one they they have a training algorithm which I don't care about that as much I mean distributed training okay lots of people have been working on that but the chief Innovation here is what they call dilation so let me uh bring that up uh so what they do is let's see hang on where did it go where did it go where's the dilation diagram all right so what
 it allows it to do a dilated attention sorry so what dilated attention allows it to do is to zoom out and take in the entire sequence all at once which controls the amount of memory uh and computation that it takes to take in that large sequence just like you and your brain zooming in and out and keeping the entire context of this image in mind at the same time and so the way that it does that is actually relatively similar to the way that human brains do it hang on hang on where did it go I'm missing the diagram okay so what they do
 is they create sparse representations and those who have been following me for a while you might remember when I came up with the idea of sparse priming representations this is something to pay attention to because what I realized is that language models only need a few Clues just a few breadcrumbs to remind it to cue in as to what is going on in the message to what's going on in the memories and this is actually why it's really good at you just give it a tiny chunk of text and it can infer the rest
 why because it has read so much that it is able to infer what came before that text and what came after and so by zooming out and creating these really sparse representations of larger sequences it can keep track of the entire thing and what it does is it will take the the up to a billion token sequence break it up slice it up and then makes a layered sparse representations of the entire thing and it will therefore be able to keep track of it now okay that sounds really nerdy uh but here's here's what it does for the performance so with
 this sparse representation with this dilation and doing it in massively in parallel it solves a few problems so one you see that the runtime stayed under a thousand uh milliseconds under one second it's more about half a second all the way up to a billion tokens so because of that it's basically zooming in and out of the text the representation of the text that it creates in the same way a very similar way that your brain keeps track of a three gigapixel image as you zoom in and out you're like okay cool okay I see a
 bunch of cars parked on the side of the road um and you can just remember that fact oh let's do a quick test what else do you remember about this image maybe you remember that the the Hollywood sign is in the background over here somewhere there it is oh no that's not it but it's somewhere in here so it's like okay based on the cars and the Arid desert and that I'm based I'm guessing that this is Los Angeles right uh anyways point being is that oh there it is Hollywood uh so this is these are the
 Hollywood Hills and you can remember oh yeah there was a nice mansion over here there's cars parked over here that's probably downtown LA the Hollywood sign is over here so by keeping by basically creating a mental map this treats gigantic pieces of information not unlike a diffusion model when and because I I got I was clued in on that when I looked at the way that it was it was um mapping everything and I was like hold on it's creating a map of the text by just breaking it down algorithmically and saying okay let's
 just make a scatter plot of all the text here uh or scatter plot's not the right word but the it's basically making a bitmap of the text of the representations of what is going on in the sequence and I'm like okay this is a fundamentally different approach to representing text and this is also really similar to some of the experiments that I've done if you remember Remo rolling episodic memory organizer which creates layers of abstraction this does it algorithmically in real time so this just blows
 everything that I've done with memory research completely out of the water it also has the ability to basically uh kind of summarize as it goes and that's not necessarily the right word because summarization means that you take one piece of text and create a smaller piece of text but this creates a neural summarization a neural network summarization by creating these layers of abstraction and this allows it to zoom in and out as it needs to so that it can cast its attention around internally in order to
 keep track of such a long sequence now okay great what does this mean as someone who has been using GPT since gpt2 where it was basically just a sentence Transformer it couldn't do a whole lot more you know like in in this model up here the original GPT was 512 tokens and gpt2 I think was what a thousand I don't remember maybe it was five uh 512 as well and then the initial version of gpt3 was 2 000 tokens we got upgraded to 4 000 tokens then we got GPT 3.
 5 and gpt4 so we're at eight thousand and sixteen thousand tokens as these attention mechanisms get bigger and as the context window gets bigger one thing that I've noticed is that there are one these are step changes in terms of algorithmic efficiencies but in terms of what they are capable of doing as I tell a lot of my uh my consultation clients do not ever try and you know get around the context window limitation because one a new model is coming out within six months that's going to completely blow open that window and two it's just a
 limitation of the model so when you can read a billion tokens which by the way humans read about one to two billion words in their entire lifetime when you have a model that can read a billion tokens in a second that is almost that is half a lifetime worth of reading and knowledge that this model can take in in a second so when you have a model that can ingest that much information suddenly retraining models doesn't matter you just give it the log of all news all events all papers whatever tasks that you're doing
 you just give it all of it at once and it can keep track of all of that text in its head in its virtual head um all at once and it can pay attention to the bits that it needs to with those sparse representations it is it is impossible for me to oversell the long-term ramifications of these kinds of algorithmic changes and so a couple months ago when I said AGI within 18 months this is the kind of trend that I was paying attention to there is no limit to the algorithmic breakthroughs we are seeing right now now that doesn't mean that there won't
 eventually be diminishing returns but at the same time we are exploring this blue ocean space and we've we've all right for those of you that have played Skyrim and other RPGs we unlocked a new map and the grayed out area is this big and we've explored this much of this new map that is how much potential there is to explore out here and the other thing is this research is accelerating there's a few reasons for that on one of the live streams like someone asked me like how do we know that this isn't an AI winter
 and I pulled up a chart that showed an exponential growth of investment where the money goes the research goes and because the money is flowing into the research it's happening what you I mean we saw the same thing with solar and literally every other disruptive technology is once the investment comes you know that the breakthroughs are going to follow it's just that simple and this is one of those kinds of breakthroughs so what does this mean uh put it this way rather than trying to you know play Tetris with memory and you
 know trying to fit 10 pounds of stuff into a five pound bag now once this becomes commercially ready which it's coming it's it's possible on paper they did it so even if we don't get a billion tokens this time next year it's coming the what this allows you to do is let's say for instance um you are working on a medical research thing and it's like okay well you know we've we've got a literature review of literally 2000 papers per month to read put all the papers in this model and and
 say tell me exactly which papers are most relevant so the the the ability for in-context learning uh is incredible and it can hold more in its brain in its mind than any 10 humans can and this is again this is not the limit imagine a year from now we're six months from now when uh long net two comes out and it's a trillion tokens or 10 trillion tokens and what they say in this paper is that maybe we're gonna see a a point very soon where it could have its context window could include basically the entire internet
 this is a step towards super intelligence make no mistake that the ability to held and use that much information in real time to produce plans to forecast to anticipate to come up with insights this is a critical step towards digital super intelligence I am not being hyperbolic here and neither is this paper when they say we could conceivably build a model that can read the entire internet in one go 
